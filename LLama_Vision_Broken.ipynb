{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPZK7Jr9Bms9VHVmpyjxwLT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graylan0/GPTVision-plugin/blob/main/LLama_Vision_Broken.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg47CdXfjxKb",
        "outputId": "94b6ee17-33ae-4577-c439-fe36280bb504"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-17 12:02:41--  https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q8_0.bin\n",
            "Resolving huggingface.co (huggingface.co)... 65.8.178.12, 65.8.178.27, 65.8.178.93, ...\n",
            "Connecting to huggingface.co (huggingface.co)|65.8.178.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/30/e3/30e3aca7233f7337633262ff6d59dd98559ecd8982e7419b39752c8d0daae1ca/3bfdde943555c78294626a6ccd40184162d066d39774bd2c98dae24943d32cc3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.ggmlv3.q8_0.bin%3B+filename%3D%22llama-2-7b-chat.ggmlv3.q8_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1700481761&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMDQ4MTc2MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zMC9lMy8zMGUzYWNhNzIzM2Y3MzM3NjMzMjYyZmY2ZDU5ZGQ5ODU1OWVjZDg5ODJlNzQxOWIzOTc1MmM4ZDBkYWFlMWNhLzNiZmRkZTk0MzU1NWM3ODI5NDYyNmE2Y2NkNDAxODQxNjJkMDY2ZDM5Nzc0YmQyYzk4ZGFlMjQ5NDNkMzJjYzM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=fmr0hskG1Zo2Yzgcc0Mg5IlOGu%7EFqApEXkWWMOt-7yWjpxIrU8U%7EIeFj2K9gCvGksTqPsPPI-yJ57Qe%7EkUcJWzeHMBJbBTeXeh4vuePoQi8V0n-aFM7vxXVwDL6syou7oMOb1X0bdvyat%7EG1EwPk4v7uY8mGdkqOptti5GXO-8fNwqNp3Edj96c6muziZi24gAcbXKihzztTwVKLPb1Qv1TT%7EpXnZhDc2yGeIal0mkoRBJfjiBhQA%7EFdzEFd%7EiQibz0R0fxPjcTscrrl2Y7glbtPb4KqnMqvgbT7jiBVKAeVGroQk3I10sPZnDY9--zow2HCFgi5JSpZUlrSigh2Qw__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-11-17 12:02:41--  https://cdn-lfs.huggingface.co/repos/30/e3/30e3aca7233f7337633262ff6d59dd98559ecd8982e7419b39752c8d0daae1ca/3bfdde943555c78294626a6ccd40184162d066d39774bd2c98dae24943d32cc3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-7b-chat.ggmlv3.q8_0.bin%3B+filename%3D%22llama-2-7b-chat.ggmlv3.q8_0.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1700481761&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcwMDQ4MTc2MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8zMC9lMy8zMGUzYWNhNzIzM2Y3MzM3NjMzMjYyZmY2ZDU5ZGQ5ODU1OWVjZDg5ODJlNzQxOWIzOTc1MmM4ZDBkYWFlMWNhLzNiZmRkZTk0MzU1NWM3ODI5NDYyNmE2Y2NkNDAxODQxNjJkMDY2ZDM5Nzc0YmQyYzk4ZGFlMjQ5NDNkMzJjYzM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=fmr0hskG1Zo2Yzgcc0Mg5IlOGu%7EFqApEXkWWMOt-7yWjpxIrU8U%7EIeFj2K9gCvGksTqPsPPI-yJ57Qe%7EkUcJWzeHMBJbBTeXeh4vuePoQi8V0n-aFM7vxXVwDL6syou7oMOb1X0bdvyat%7EG1EwPk4v7uY8mGdkqOptti5GXO-8fNwqNp3Edj96c6muziZi24gAcbXKihzztTwVKLPb1Qv1TT%7EpXnZhDc2yGeIal0mkoRBJfjiBhQA%7EFdzEFd%7EiQibz0R0fxPjcTscrrl2Y7glbtPb4KqnMqvgbT7jiBVKAeVGroQk3I10sPZnDY9--zow2HCFgi5JSpZUlrSigh2Qw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.157.162.99, 108.157.162.27, 108.157.162.95, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.157.162.99|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7160799872 (6.7G) [application/octet-stream]\n",
            "Saving to: ‘llama-2-7b-chat.ggmlv3.q8_0.bin’\n",
            "\n",
            "llama-2-7b-chat.ggm 100%[===================>]   6.67G   219MB/s    in 36s     \n",
            "\n",
            "2023-11-17 12:03:17 (192 MB/s) - ‘llama-2-7b-chat.ggmlv3.q8_0.bin’ saved [7160799872/7160799872]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7D6_Bw9Mnhfr",
        "outputId": "f65cd44b-c9a1-45e7-9cf8-3f4013d61e04"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.35.2-py3-none-any.whl (7.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.9/7.9 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.19.4-py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.7/311.7 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.19.4 safetensors-0.4.0 tokenizers-0.15.0 transformers-4.35.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qcZc3EunVbk",
        "outputId": "3036ddc9-3f3c-4725-d43e-31a243e9bd8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python==0.1.78\n",
            "  Downloading llama_cpp_python-0.1.78.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/1.7 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.1.78) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.78-cp310-cp310-linux_x86_64.whl size=5822213 sha256=e50dcc5ba0a987be15b80c816e1a4692c615c33e991e47700d55d7f336a77628\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/f9/20/9ca660a9d3f2a47e44217059409478865948b5c8a1cba70030\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.1.78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import base64\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from llama_cpp import Llama\n",
        "import logging\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "llm = Llama(model_path=\"llama-2-7b-chat.ggmlv3.q8_0.bin\", n_gpu_layers=-1, n_ctx=3900)\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "def encode_image_to_base64(image_data):\n",
        "    image = Image.open(BytesIO(image_data))\n",
        "    if image.mode == 'RGBA':\n",
        "        image = image.convert('RGB')\n",
        "    image.thumbnail((512, 512), image.thumbnail((512, 512), Image.Resampling.LANCZOS))\n",
        "    buffer = BytesIO()\n",
        "    image.save(buffer, format=\"JPEG\")\n",
        "    return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
        "\n",
        "def resize_image(image_data):\n",
        "    try:\n",
        "        image = Image.open(BytesIO(image_data))\n",
        "        if image.mode == 'RGBA':\n",
        "            image = image.convert('RGB')\n",
        "        image.thumbnail((512, 512), Image.ANTIALIAS)\n",
        "        buffer = BytesIO()\n",
        "        image.save(buffer, format=\"JPEG\")\n",
        "        return buffer.getvalue()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error resizing image: {e}\")\n",
        "        return None\n",
        "\n",
        "def multi2vec_process_image(image_url, weaviate_instance_url, class_name):\n",
        "    try:\n",
        "        response = requests.get(image_url)\n",
        "        if response.status_code != 200:\n",
        "            logging.error(f\"Failed to download image: HTTP {response.status_code}\")\n",
        "            return None\n",
        "        image_data = response.content\n",
        "        resized_image_data = resize_image(image_data)\n",
        "        if resized_image_data is None:\n",
        "            return None\n",
        "        base64_encoded_image = encode_image_to_base64(resized_image_data)\n",
        "        data_object = {\"class\": class_name, \"properties\": {\"image\": base64_encoded_image}}\n",
        "        response = requests.post(f\"{weaviate_instance_url}/objects\", json=data_object)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            logging.error(f\"Failed to vectorize image: HTTP {response.status_code}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in multi2vec_process_image: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_clip_summary(image_data):\n",
        "    try:\n",
        "        # Load the image\n",
        "        image = Image.open(BytesIO(image_data))\n",
        "\n",
        "        # Prepare the inputs for the CLIP model\n",
        "        # Note: We're using a dummy text input here because CLIP expects both text and image inputs.\n",
        "        # You can replace \"a photo\" with a more suitable placeholder text if needed.\n",
        "        inputs = clip_processor(text=[\"a photo\"], images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "        # Get the outputs from the CLIP model\n",
        "        outputs = clip_model(**inputs)\n",
        "\n",
        "        # Process the outputs\n",
        "        logits_per_image = outputs.logits_per_image\n",
        "        probs = logits_per_image.softmax(dim=1)\n",
        "\n",
        "        # Return the most probable label (as an example)\n",
        "        return probs.argmax().item()\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in get_clip_summary: {e}\")\n",
        "        return None\n",
        "def calculate_token_usage(prompt, vectorized_data, max_tokens=3999):\n",
        "    prompt_tokens = len(prompt.split())\n",
        "    available_tokens = max_tokens - prompt_tokens\n",
        "    tokens_per_number = 1\n",
        "    max_data_length = available_tokens // tokens_per_number\n",
        "    return max_data_length\n",
        "\n",
        "def intelligent_chunking(vectorized_data, max_chunk_size):\n",
        "    return [vectorized_data[i:i + max_chunk_size] for i in range(0, len(vectorized_data), max_chunk_size)]\n",
        "\n",
        "def llama2_process_vectorized_image(vectorized_image, custom_prompt):\n",
        "    max_data_length = calculate_token_usage(custom_prompt, vectorized_image['vector'])\n",
        "    chunks = intelligent_chunking(vectorized_image['vector'], max_data_length)\n",
        "    processed_outputs = []\n",
        "    for index, chunk in enumerate(chunks):\n",
        "        rules_based_prompt = (\n",
        "            f\"<sys>You are inspecting an Image please describe it.{index+1}</sys> \"\n",
        "            f\"[INST]Adhere to the following Image Inspection AI Vector Space rules: \"\n",
        "            \"1. Analyze the data for patterns. \"\n",
        "            \"2. Identify key features and characteristics. \"\n",
        "            \"3. Provide a descriptive analysis of the image content. \"\n",
        "            \"4. Ensure accuracy and clarity in the description. \"\n",
        "            f\"Chunk Data: {json.dumps(chunk)}[/INST]\"\n",
        "        )\n",
        "        combined_prompt = f\"{custom_prompt}. {rules_based_prompt}\"\n",
        "        logging.debug(f\"Prompt for Llama model: {combined_prompt}\")\n",
        "        try:\n",
        "            output = llm(combined_prompt, max_tokens=300, stop=[\"\\n\"], echo=True)\n",
        "            if 'text' in output:\n",
        "                processed_outputs.append(output['text'])\n",
        "            else:\n",
        "                logging.error(f\"No text found in output for chunk {index+1}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing chunk {index+1} with Llama model: {e}\")\n",
        "    return ' '.join(processed_outputs)\n",
        "\n",
        "weaviate_instance_url = \"https://roughly-sure-whale.ngrok-free.app/v1\"\n",
        "class_name = \"BindExample\"\n",
        "image_url = \"https://user-images.githubusercontent.com/34530588/258687329-1a9f08a3-f092-4180-a854-4ab48a04cf99.png\"\n",
        "\n",
        "response = requests.get(image_url)\n",
        "if response.status_code == 200:\n",
        "    image_data = response.content\n",
        "    clip_summary = get_clip_summary(image_data)\n",
        "    vectorized_image = multi2vec_process_image(image_url, weaviate_instance_url, class_name)\n",
        "    if vectorized_image:\n",
        "        vectorized_image['clip_summary'] = clip_summary\n",
        "        custom_prompt = \"Describe the content of this image\"\n",
        "        processed_data = llama2_process_vectorized_image(vectorized_image, custom_prompt)\n",
        "        if processed_data:\n",
        "            print(processed_data)\n",
        "        else:\n",
        "            print(\"Failed to process image data.\")\n",
        "    else:\n",
        "        print(\"Failed to vectorize image.\")\n",
        "else:\n",
        "    logging.error(f\"Failed to download image: HTTP {response.status_code}\")\n",
        "    print(\"Failed to retrieve image data.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cITKzTKdjxNC",
        "outputId": "fa9c0e39-2c06-4f70-dbc7-34ebcd4e81dd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n",
            "<ipython-input-1-610f349b8b2b>:30: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
            "  image.thumbnail((512, 512), Image.ANTIALIAS)\n",
            "ERROR:root:No text found in output for chunk 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to process image data.\n"
          ]
        }
      ]
    }
  ]
}